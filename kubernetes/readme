üîµ 1. Difference between Azure CNI and Kubenet? Which one do you use and why?

This version is easy to understand, professional, and exactly what interviewers expect from an Azure DevOps Engineer.

‚≠ê Interview Answer (Best Version):

In AKS, we have two networking models: Kubenet and Azure CNI, and they differ mainly in how pod IPs are assigned and how the cluster integrates with the VNet.

Kubenet is the basic/legacy option. Nodes get IPs from the Azure VNet, but pods get IPs from an internal Kubernetes network, not from the VNet.
Because pods don't have VNet IPs, any communication from pods to VNet resources (SQL, Storage, APIs, on-prem) goes through SNAT, which creates routing limitations.
Kubenet uses fewer IPs and is good for small clusters but is not suitable for enterprise-grade networking.

Azure CNI, on the other hand, assigns real VNet IPs directly to pods.
This means pods become first-class members of the VNet and can directly communicate with Private Endpoints, on-prem networks, firewalled resources, or internal services without NAT.
It gives better performance, easier troubleshooting, full network visibility, NSG/UDR support, and enterprise security.

I prefer Azure CNI in production because most enterprise workloads need to access Azure PaaS services like SQL, Storage, Key Vault over Private Endpoints, and require secure VNet-level routing.
With CNI, pod traffic flows naturally in the VNet, and we don‚Äôt face SNAT or routing issues like in Kubenet.

‚≠ê Short Summary to Speak at the End

So in simple terms:

Kubenet ‚Üí Overlay network, limited routing, uses fewer IPs

Azure CNI ‚Üí Full VNet integration, pod gets VNet IP, best for enterprise workloads

That‚Äôs why I use Azure CNI for production AKS clusters.

‚≠ê Real Example (You Can Say This in Interview)

In one of my projects, we faced connectivity issues because pods using Kubenet couldn't properly communicate with SQL Private Endpoint.
After moving to Azure CNI, pods received VNet IPs, and Private Endpoint traffic started working smoothly.
This improved both network reliability and security.

‚≠ê If interviewer asks deeper: ‚ÄúAzure CNI uses more IPs‚Äîhow do you manage it?‚Äù

You answer:

Yes, Azure CNI consumes more IPs because each pod gets a VNet IP.
To avoid subnet exhaustion, Azure now provides Azure CNI Overlay, where pods get overlay IPs while nodes use VNet IPs.

This gives the benefits of CNI without large IP requirements.


2. What ingress controllers have you used in AKS?

Explain NGINX vs AGIC and when to use which.**

‚≠ê Interview Answer (Best Version):

In AKS, I have worked with two main ingress controllers:
NGINX Ingress Controller and AGIC (Application Gateway Ingress Controller).

NGINX Ingress Controller runs inside the AKS cluster as a deployment.
It uses an internal Azure Load Balancer or public load balancer to receive traffic.
It‚Äôs lightweight, easy to configure, and suitable for most microservices running on HTTP/HTTPS.

AGIC (Application Gateway Ingress Controller) integrates AKS with Azure Application Gateway.
Application Gateway becomes the data plane and provides enterprise-grade features like:

WAF (Web Application Firewall)

SSL/TLS termination

URL routing

End-to-end TLS

DDoS protection

I choose NGINX for typical microservices where quick deployments and flexibility are required.
I choose AGIC when we need enterprise security, WAF protection, compliance, or advanced Layer 7 features.

| Feature           | NGINX Ingress Controller                  | AGIC (App Gateway Ingress Controller)   |
| ----------------- | ----------------------------------------- | --------------------------------------- |
| Runs Where?       | Inside AKS                                | Azure Application Gateway               |
| WAF Support       | No (unless 3rd party)                     | Yes, built-in WAF                       |
| TLS               | Terminated at NGINX                       | Terminated at App Gateway               |
| Performance       | Very fast                                 | Enterprise-grade, scalable              |
| Path/Host Routing | Yes                                       | Yes                                     |
| Best For          | Microservices, dev/test, simple prod apps | Secure enterprise workloads, compliance |


Private Endpoints in AKS (Important Point)

Private Endpoints are used when the application inside AKS needs to connect securely to Azure PaaS services without going over public internet.

Examples:

Azure SQL

Storage Account

Key Vault

Cosmos DB

With Azure CNI, pods get VNet IPs, so they can directly communicate with these private endpoints using purely internal traffic.

1Ô∏è‚É£ Why do we not use NodePort in production?

It‚Äôs not secure, difficult to manage, and exposes the node directly.

2Ô∏è‚É£ Why do we prefer Ingress over LoadBalancer?

Ingress gives a single entry point with routing, TLS, and WAF support, while LoadBalancer exposes only one service.

3Ô∏è‚É£ Why are private endpoints important in AKS?

They allow AKS pods to communicate privately with Azure services without public exposure, improving security and compliance.


1. HPA Manifest (Horizontal Pod Autoscaler)

This example scales your deployment from 2 to 10 pods based on CPU usage (70%).

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: myapp-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

‚≠ê Requirements for HPA to work:

Metrics Server must be installed

Deployment must define resources.requests for CPU

Target must use apps/v1 ‚Üí Deployment/StatefulSet/ReplicaSet

‚≠ê 2. Node Autoscaling (AKS Cluster Autoscaler)

Important:
Cluster Autoscaler is not configured through Kubernetes YAML.
It is configured at the AKS nodepool level using Azure CLI or Portal.

Here is the node pool autoscaling manifest in ARM/Bicep style, but I will also give Azure CLI commands, because that's what is used in real projects.

‚≠ê AKS Node Pool Autoscaling ‚Äì Azure CLI (Real Method)
‚úî Enable autoscaling on a node pool
az aks nodepool update \
  --resource-group myRG \
  --cluster-name myAKS \
  --name nodepool1 \
  --enable-cluster-autoscaler \
  --min-count 2 \
  --max-count 5

‚úî Disable cluster autoscaler
az aks nodepool update \
  --resource-group myRG \
  --cluster-name myAKS \
  --name nodepool1 \
  --disable-cluster-autoscaler

‚úî Create a node pool with autoscaling enabled
az aks nodepool add \
  --resource-group myRG \
  --cluster-name myAKS \
  --name appnp \
  --node-count 2 \
  --enable-cluster-autoscaler \
  --min-count 2 \
  --max-count 6

‚≠ê Bicep/ARM Manifest for Node Autoscaling (Optional)

You can show this if interviewer asks for IaC configuration.

properties:
  orchestratorVersion: "1.29.0"
  mode: "User"
  enableAutoScaling: true
  minCount: 2
  maxCount: 6
  vmSize: "Standard_D4s_v3"


This is how node autoscaling is represented in IaC.

‚≠ê Bonus ‚Äî Combined Example Deployment + HPA (Very Useful for Interview)

Here is a minimal deployment that works perfectly with the HPA above.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myacr.azurecr.io/myapp:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: "200m"
          limits:
            cpu: "500m"

Why this matters?

HPA will NOT work unless you define CPU requests/limits.

Q) how do you troubleshoot situation when frontend communicate with backend?
In that situation, I treat it as an end-to-end path problem and debug it layer by layer from the frontend down to the backend. 
First, I check what the frontend is actually seeing in the browser DevTools (Network tab): is it a 4xx (CORS/auth/URL issue), 
5xx (backend error), timeout, or DNS error? I confirm the exact URL, port, and protocol the frontend is calling (HTTP vs HTTPS, 
path like /api/orders, correct domain). Next, I test the backend directly: use curl or Postman against the same endpoint (ingress/load 
balancer URL) to see if the issue is only from browser or from everywhere. If the backend is running in Kubernetes, I then check the 
Ingress and Service: kubectl describe ingress to confirm it routes to the right service/port, kubectl get svc and kubectl get endpoints 
to make sure there are healthy pods behind the service, and kubectl get pods / kubectl logs to see if backend pods are crashing, not ready, 
or throwing errors. I also look for common issues like wrong service targetPort, readiness probe failing (so ingress has no ready backends), 
CORS misconfiguration (browser blocking), or auth failures (401/403). If network policies or firewalls are in place, I verify they allow traffic 
from the frontend (namespace/IP) to the backend service. In short, I start at the browser (error type), then check ingress/load balancer, then 
service/endpoints, then backend pod logs and configuration, and that step-by-step approach quickly shows whether it‚Äôs a frontend config issue, 
routing issue, or a real backend problem.


Explain your production-grade Kubernetes architecture.

Here is a clear, interview-ready explanation of a production-grade Kubernetes (AKS) architecture, written in paragraph format, 
exactly how you can speak it in an interview.

In production, our Kubernetes architecture is designed for high availability, security, scalability, and observability. We run 
AKS in a custom Azure VNet using Azure CNI, so pods get VNet IPs and can securely communicate with other Azure resources. The cluster 
is deployed across multiple availability zones to avoid single points of failure. We separate workloads using multiple namespaces (for 
example dev, test, prod, or team-based namespaces) and apply RBAC and network policies for isolation and security.

For compute, we use multiple node pools: a system node pool dedicated to Kubernetes system components and one or more user node pools for 
application workloads. User node pools are sized based on workload needs, sometimes with separate pools for CPU-intensive, memory-intensive, 
or spot workloads. Autoscaling is enabled using HPA for pod scaling and Cluster Autoscaler for node scaling, so the platform can handle traffic 
spikes and scale down during low usage.

For traffic management, external access is handled through an Ingress Controller (NGINX or Application Gateway Ingress Controller). This provides a 
single entry point with TLS termination, path/host-based routing, and optional WAF. Internal communication between services is done using ClusterIP 
services. Outbound and hybrid connectivity follows VNet routing, NSGs, and UDRs, and access to Azure PaaS services like Key Vault, Storage, or SQL is 
done through Private Endpoints to avoid public exposure.

From a security perspective, we avoid hard-coded secrets and use Managed Identity or Azure Workload Identity so pods can securely access Azure services. 
Secrets are stored in Azure Key Vault and integrated with AKS. We also use PodDisruptionBudgets, readiness/liveness probes, and proper resource requests 
and limits to ensure stability during upgrades and failures.

For observability, we enable Azure Monitor / Container Insights, centralize logs in Log Analytics, and create alerts for pod restarts, high CPU/memory, 
node health, and ingress errors. CI/CD is handled through Azure DevOps, where pipelines build images, push to a registry, and deploy using Kubernetes 
manifests or Helm with controlled rollouts and rollback support.

In summary, a production-grade Kubernetes architecture combines multi-AZ AKS, secure networking, ingress-based traffic routing, autoscaling, strong 
identity and RBAC, and full monitoring, ensuring the platform is secure, resilient, and easy to operate at scale.

What happens when readiness/liveness probes fail?
When a readiness probe fails, Kubernetes does not restart the container. Instead, it marks the pod as NotReady and immediately removes it from the 
Service or Ingress load-balancer endpoints. This means no new traffic is sent to that pod, but the container continues running in the background. 
Once the readiness probe starts passing again, the pod is added back to the load balancer and begins receiving traffic.

When a liveness probe fails, Kubernetes assumes the application is unhealthy and restarts the container. The kubelet kills the container and starts 
it again according to the pod‚Äôs restart policy. If the application keeps failing the liveness probe after restarting, the pod enters a CrashLoopBackOff 
state, where Kubernetes backs off and retries restarting it with increasing delays.


How do you implement autoscaling (HPA/VPA/Cluster Autoscaler)?
In Kubernetes, autoscaling is implemented at three levels: pod level using HPA, resource level using VPA, and node level using Cluster Autoscaler. 
For Horizontal Pod Autoscaler (HPA), I first ensure the Metrics Server or monitoring stack is enabled, then define CPU, memory, or custom metrics and 
set min/max replicas in the HPA configuration. HPA continuously monitors metrics and automatically increases or decreases the number of pod replicas 
based on load, which is the most common and production-ready autoscaling method.

For Vertical Pod Autoscaler (VPA), I use it mainly to optimize resource requests and limits. VPA observes pod usage over time and recommends or 
automatically adjusts CPU and memory values. Because VPA usually requires pod restarts when applying changes, I mostly run it in recommendation 
mode in production and use those recommendations to fine-tune resource settings, rather than letting it automatically restart pods.

At the infrastructure level, I enable Cluster Autoscaler on AKS node pools. Cluster Autoscaler automatically adds nodes when pods cannot be scheduled 
due to insufficient CPU or memory, and removes underutilized nodes when demand decreases. This works together with HPA: HPA scales pods first, and if 
there is not enough node capacity, Cluster Autoscaler scales the nodes.

In summary, HPA handles traffic-based scaling, VPA optimizes pod resource sizing, and Cluster Autoscaler ensures enough node capacity, and together 
they provide a fully elastic, cost-efficient, production-grade autoscaling solution in AKS.
